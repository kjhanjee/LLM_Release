"""Module for creating trainer class to train the Pytorch LLM model
"""
# Standard imports
import os
import torch
from tokenizers import Tokenizer
import numpy as np
from tqdm import tqdm
from IPython.display import clear_output
from utils.training_data_generator import TrainingDataGen
from utils.read_config import get_config

config = get_config()


class Trainer:
    """Module for creating trainer class to train the Pytorch LLM model
    """
    def __init__(self, model, tokenizer: Tokenizer, optimizer:torch.optim.Optimizer = None, max_sequence_length: int = 4096, checkpoint_path: str = None, batch_size: int = 2):
        """Init Function to initialize class variables
        
        Args:
            model (LLMModel): Model for training
            tokenizer (Tokenizer): Tokenizer to be used for the model
            optimizer (Optimizer): Optimizer for gradient calculations
            max_sequence_length (int): Maximum sequence length supported by the model
            checkpoint_path (str): Checkpoint path where the artefacts need to be stored for training
            batch_size (int): Batch Size for training the Model
            
        Returns:
            None
        """
        super().__init__()
        self.tokenizer = tokenizer
        self.model = model
        self.loss_function = torch.nn.CrossEntropyLoss()
        self.max_sequence_length = max_sequence_length
        self.step = 1
        if optimizer is None:
            self.optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)
        else:
            self.optimizer = optimizer
        self.last_epoch = 0
        self.last_loss = None
        self.last_sentence_index = 0
        self.batch_size = batch_size
        if checkpoint_path:
            if os.path.exists(checkpoint_path):
                checkpoint = torch.load(checkpoint_path)
                self.model.load_state_dict(checkpoint['model_state_dict'])
                self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
                self.last_epoch = checkpoint['epoch']
                self.last_loss = checkpoint['loss']
                self.step = checkpoint['step']
        self.input_tensor = None
        self.output = None
    
    def sentence_iterator(self,sentences):
        """Function to iterate over sentences generated by the Data generator
        
        Args:
            sentences (list): List of Sentences/Texts from the file currently the model is training on
            
        Yields:
            list (list): Yields Batch of Training data for the current training step
        """
        sentences = self.tokenizer.encode_batch(sentences)
        sequences = []
        for item in sentences:
            ids = item.ids
            if len(ids) < self.max_sequence_length:
                for _ in range(0,self.max_sequence_length - len(ids) + 1):
                    ids.append(24) 
                sequences.append(ids)
                if len(sequences) == self.batch_size:
                    yield sequences
                    sequences = []
            else:
                i = 0
                while i < len(ids) - 1:
                    if i + self.max_sequence_length < len(ids) - 1:
                        ids2 = ids[i:i+self.max_sequence_length+1]
                        sequences.append(ids2)
                    else:
                        ids2 = ids[i:]
                        for _ in range(0,self.max_sequence_length-len(ids2) + 1):
                            ids2.append(24)
                        sequences.append(ids2)
                    i = i + 1
                    if len(sequences) == self.batch_size:
                        sequences2 = sequences
                        sequences = []
                        yield sequences2
                        
            self.last_sentence_index += 1
                    
    def train(self, data: TrainingDataGen, epochs:int, batch_size:int):
        """Function to train the given model
        
        Args:
            data (TrainingDataGen): Object of TrainingDataGen class as a batch generator
            epochs (int): Number of Epochs for which the model will be trained
            batch_size (int): Number of Batches per step to be provided to the model
            
        Yields:
            LLMModel: Returns trained Model after traiing session
        """
        continue_do = 1
        accum_iter = 3
        with tqdm(total=100) as pbar:
            for epoch in tqdm(range(epochs),total = epochs):
                if epoch == self.last_epoch:
                    loss = self.last_loss
                    continue_do = 0
                if continue_do == 1:
                    continue

                step = self.step
                for sentences_from_file in data.gen_batch():
                    self.last_sentence_index = 0
                    for sequences2 in self.sentence_iterator(sentences_from_file):
                        self.model.train()

                        input_tensor = torch.zeros((batch_size, self.model.max_sequence_length), dtype=torch.long)
                        mask_tensor = torch.zeros((batch_size, self.model.max_sequence_length), dtype=torch.long)
                        target_tensor = torch.zeros((batch_size, self.model.max_sequence_length), dtype=torch.long)
                        
                        for i, input_entry in enumerate(sequences2):
                            input_tensor[i] = torch.from_numpy(np.array(input_entry[:-1]))

                        for i, mask_entry in enumerate(sequences2):
                            mask_tensor[i] = torch.from_numpy(np.array(mask_entry[:-1]))
                            if len((mask_tensor[i]==24).nonzero(as_tuple=True)[0].size()) == 0:
                                mask_tensor[i] = mask_tensor[i][(mask_tensor[i]==24).nonzero(as_tuple=True)[0]:] = 0
                                mask_tensor[i] = mask_tensor[i][:(mask_tensor[i]==24).nonzero(as_tuple=True)[0]] = 1
                            else:
                                 mask_tensor[i] = torch.from_numpy(np.array([1 for item in range(0,self.max_sequence_length)]))
                        
                        for i, target_entry in enumerate(sequences2):
                            target_tensor[i] = torch.from_numpy(np.array(target_entry[1:]))

                                        
                        # Compute the model output
                        clear_output(wait=True)
                        pbar.display()
                        print("Current File: ", data.last_file)
                        print(f"Step {step} Input Tensor: "+str([self.tokenizer.decode([int(item.detach().cpu().numpy()) for item in tensor if not item.detach().cpu().numpy()==24],skip_special_tokens = False) for tensor in input_tensor]))
                        print(f"\n\nStep {step} Target Tensor: "+str([self.tokenizer.decode([int(item.detach().cpu().numpy()) for item in tensor if not item.detach().cpu().numpy()==24],skip_special_tokens = False) for tensor in target_tensor]))
                        

                        model_output = self.model.forward(x=input_tensor, mask=mask_tensor)
                        out = model_output.clone().detach()

                        # Compute the losses
                        # The loss is computed on the model output and the target
                        loss = self.loss_function(model_output.transpose(1, 2), target_tensor)
                            
                        loss = loss / accum_iter
                            
                        outputs = [[] for counter in range(batch_size)]
                        for index, tensor in enumerate(torch.softmax(out,dim=-1)):
                            for item in tensor:
                                if not int(np.argmax(item.detach().cpu().numpy()))==24:
                                    outputs[index].append(int(np.argmax(item.detach().cpu().numpy())))

                        
                        
                        print(f'\n\nStep {step} Loss: ', loss.item())
                        print(f"\n\nStep {step} Output: "+str(self.tokenizer.decode_batch(outputs,skip_special_tokens=False)))
                        
                        loss.backward()                     
                        torch.nn.utils.clip_grad_norm_(self.model.parameters(), 0.5)                   
                            
                        
                        if step % accum_iter == 0:
                            self.optimizer.step()
                            self.optimizer.zero_grad(set_to_none=True)  
                            PATH = config['checkpoint_path']+"model"+str(step)+".pt"
                            print("Saving shit!")
                            torch.save({
                                        'epoch': epoch,
                                        'step':step,
                                        'model_state_dict': self.model.state_dict(),
                                        'optimizer_state_dict': self.optimizer.state_dict(),
                                        'loss': loss,
                                        "last_sentence_index":self.last_sentence_index
                                        }, PATH)
                            if os.path.exists(config['checkpoint_path']+"model"+str(step-3)+".pt"):
                                os.remove(config['checkpoint_path']+"model"+str(step-3)+".pt")
                            data.save_state(config['checkpoint_path'])
                            print("All Saved!")
                            
                        step += 1
                        self.step = step
                print('Epoch:', epoch, 'Loss:', loss)
                self.step = 1
                pbar.update(1)

        return self.model